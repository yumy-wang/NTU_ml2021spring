{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw7_tutorial",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvSGDbExff_I"
      },
      "source": [
        "# **A toy example for HW7 Bert QA**\n",
        "\n",
        "If you have any questions, feel free to email us at ntu-ml-2021spring-ta@googlegroups.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64N6tucFQRl8"
      },
      "source": [
        "# Install transformers\n",
        "Documentation for the toolkit:　https://huggingface.co/transformers/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7etZIyfmCVg"
      },
      "source": [
        "!pip install transformers==4.5.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLSGMP5wQXOY"
      },
      "source": [
        "# Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdGJ3hFN_8Q5"
      },
      "source": [
        "import torch\n",
        "from transformers import AdamW, BertTokenizerFast, BertForQuestionAnswering"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11qNo2EPQhuN"
      },
      "source": [
        "# Load Model and Tokenizer\n",
        "A list of avaliable pretrained models: https://huggingface.co/models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xODRE-8RmMoE"
      },
      "source": [
        "# model_name can be either: models in huggingface model hub or models saved using save_pretrained\n",
        "model_name = 'bert-base-chinese'\n",
        "model = BertForQuestionAnswering.from_pretrained(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln-4faF8Q1h5"
      },
      "source": [
        "chi_tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
        "eng_tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQ5hBatuQ9Ix"
      },
      "source": [
        "# Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5PRNQTwqZZd"
      },
      "source": [
        "chi_paragraph = '李宏毅幾班大金。2021 ML'\n",
        "tokens = chi_tokenizer.tokenize(chi_paragraph)\n",
        "print(tokens)\n",
        "chi_tokenizer.convert_tokens_to_ids(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbtifQRwqouP"
      },
      "source": [
        "eng_paragraph = 'Lee Hung-yi which class Daikin.'\n",
        "tokens = eng_tokenizer.tokenize(eng_paragraph)\n",
        "print(tokens)\n",
        "eng_tokenizer.convert_tokens_to_ids(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuq3PhS3RCdv"
      },
      "source": [
        "# Encode vs Decode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMX-gu57ssi8"
      },
      "source": [
        "question = '李宏毅幾班?'\n",
        "paragraph = '李宏毅幾班大金。'\n",
        "encoded = chi_tokenizer.encode(question, paragraph)\n",
        "decoded = chi_tokenizer.decode(encoded)\n",
        "print(encoded)\n",
        "print(decoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BzNV4smRJ2X"
      },
      "source": [
        "# Model Inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoITv6O0tCVB"
      },
      "source": [
        "inputs = chi_tokenizer(question, paragraph, return_tensors='pt')\n",
        "# Indices of input sequence tokens in the vocabulary\n",
        "print('Input ids:      ', inputs['input_ids'])\n",
        "# Segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]\n",
        "print('Token type ids: ', inputs['token_type_ids'])\n",
        "# Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]\n",
        "print('Attention mask: ', inputs['attention_mask'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFH-Fb6LVG-v"
      },
      "source": [
        "# Testing (Chinese)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sRQGEesjBmG"
      },
      "source": [
        "question = '李宏毅幾班?'\n",
        "paragraph = '李宏毅幾班大金。'\n",
        "inputs = chi_tokenizer(question, paragraph, return_tensors='pt')\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(**inputs)\n",
        "# output = model(input_ids=inputs['input_ids'], token_type_ids=inputs['token_type_ids'], attention_mask=inputs['attention_mask'])\n",
        "\n",
        "print(\"start_logits: \")\n",
        "print(output.start_logits)\n",
        "\n",
        "print(\"end_logits: \")\n",
        "print(output.end_logits)\n",
        "\n",
        "start = torch.argmax(output.start_logits)\n",
        "end = torch.argmax(output.end_logits)\n",
        "print(\"start position: \", start.item())\n",
        "print(\"end position:   \", end.item())\n",
        "\n",
        "predict_id = inputs['input_ids'][0][start : end + 1]\n",
        "print(\"predict_id:     \", predict_id)\n",
        "\n",
        "predict_answer = chi_tokenizer.decode(predict_id)\n",
        "print(\"predict_answer: \", predict_answer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBGQCyCZUwaL"
      },
      "source": [
        "# Training (Chinese)\n",
        "For Question Answering, loss is the sum of cross entropy between the model prediction and correct answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dP3SRdS8wedu"
      },
      "source": [
        "output = model(**inputs, start_positions=torch.tensor([13]), end_positions=torch.tensor([14]))\n",
        "print(\"loss: \", output.loss)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
        "output.loss.backward()\n",
        "optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awNHFgkCyj9F"
      },
      "source": [
        "# Testing (English)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt0Glg0yyfg6"
      },
      "source": [
        "question = \"Why does Jeanie like Tom?\"\n",
        "paragraph = \"Jeanie likes Tom because Tom is good at deep learning.\"\n",
        "inputs = eng_tokenizer(question, paragraph, return_tensors='pt')\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(**inputs)\n",
        "# output = model(input_ids=inputs['input_ids'], token_type_ids=inputs['token_type_ids'], attention_mask=inputs['attention_mask'])\n",
        "\n",
        "print(\"start_logits: \")\n",
        "print(output.start_logits)\n",
        "\n",
        "print(\"end_logits: \")\n",
        "print(output.end_logits)\n",
        "\n",
        "start = torch.argmax(output.start_logits)\n",
        "end = torch.argmax(output.end_logits)\n",
        "print(\"start position: \", start.item())\n",
        "print(\"end position:   \", end.item())\n",
        "\n",
        "predict_id = inputs['input_ids'][0][start : end + 1]\n",
        "print(\"predict_id:     \", predict_id)\n",
        "\n",
        "predict_answer = eng_tokenizer.decode(predict_id)\n",
        "print(\"predict_answer: \", predict_answer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQB51L0-yn9k"
      },
      "source": [
        "# Training (English)\n",
        "For Question Answering, loss is the sum of cross entropy between the model prediction and correct answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82AZ3K7e8ZjS"
      },
      "source": [
        "output = model(**inputs, start_positions=torch.tensor([14]), end_positions=torch.tensor([19]))\n",
        "print(\"loss: \", output.loss)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
        "output.loss.backward()\n",
        "optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}