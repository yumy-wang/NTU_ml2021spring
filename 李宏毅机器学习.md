# 课程简介

## introduction

机器学习的三个步骤：

1. 定义模型：定义带未知量的函数
2. 定义损失函数：
3. 优化：找参数最优值，如gradient descent

机器学习=找函数f()，大致分类

- 二分类：输出是与否
- 回归：输出标量
- 多分类：CNN输出分类

怎么告诉机器需要的函数：

- 监督学习：labeled data
  - 计算Loss
  - 机器会自动找出loss最低的情况
- 强化学习：alphaGo——监督学习之上强化学习
- 无监督学习

机器怎样找出你想要的函数？

## rule

- git
- github
- Ubuntu环境下pyenv配置



## gradient and error

### gradient：梯度下降法

### error来自：

- variance：方差造成的偏差，理解为预测值之间的偏差，训练集误差小，测试集大==过拟合
  - 解决：更多data；正则化——让曲线更加平滑
- bias：均值造成的偏差，理解为预测与真实值之间的偏差，训练集误差大==欠拟合
  - 

<img src="https://yumytest.oss-cn-chengdu.aliyuncs.com/img/image-20201027204600646.png" alt="image-20201027204600646" style="zoom:80%;" />

# 深度学习

## 简介

怎样提升准确度？

<img src="https://yumytest.oss-cn-chengdu.aliyuncs.com/img/image-20210309152535164.png" alt="image-20210309152535164" style="zoom: 67%;" />

- 过拟合：
  - 更多训练数据
  - 数据增强（翻转、裁剪）
  - 限制模型：减少参数、参数共享（CNN）、减少特征、提前结束、正则化、dropout
- 交叉验证：N-fold Cross Validation
- mismatch：训练集和测试集分布不一样

## 优化——梯度消失

- 梯度消失：当走到梯度为0的地方，训练几乎停止

  - 鞍点：微分为0的点，可解决

  - 局部最优：local minima/maxima，不可解决

    <img src="https://yumytest.oss-cn-chengdu.aliyuncs.com/img/image-20210309145858695.png" alt="image-20210309145858695" style="zoom:67%;" />

- 判断方法：Hessian矩阵是二次微分矩阵

  <img src="https://yumytest.oss-cn-chengdu.aliyuncs.com/img/image-20210312155725425.png" alt="image-20210312155725425" style="zoom:80%;" />

  - 当我们抵达critical point即梯度为0的时候，绿色这一项为0，可以通过红色部分来判断当前是局部最优还是鞍点

  - 很简单，我们可以分三种情况，如图很直观地指出了分类方法：每种类别的第二行是等价条件，可以通过特征值来判断是否正定

    <img src="https://yumytest.oss-cn-chengdu.aliyuncs.com/img/image-20210312160339857.png" alt="image-20210312160339857" style="zoom:80%;" />

  - 通过鞍点的特征向量来计算loss减小的方向（实作中几乎没人这么做）

  

- 从经验上看：鞍点更常见

## 优化——训练提示

### batch

为什么要用batch

- 实验表明，batch_size越大，精确度会下降。一种可能的解释是batch训练时的loss函数不一样，因而遇到鞍点时可以继续训练

  <img src="https://yumytest.oss-cn-chengdu.aliyuncs.com/img/image-20210309153907587.png" alt="image-20210309153907587" style="zoom:67%;" />

- 实验表明，小批量测试集表现也更好（泛化性）

- 劣势：batch_size越大，每一epoc用时越大，总用时越少

### momentum

- 类比物理中的动量、惯性，每一次梯度更新方向还要考虑前一次梯度方向

  <img src="https://yumytest.oss-cn-chengdu.aliyuncs.com/img/image-20210309155155708.png" alt="image-20210309155155708" style="zoom:80%;" />

### Adaptive Learning Rate

- 怎样选择学习率？？

- 原则：梯度变化平缓，学习率设置大一点；反之，小一点

  <img src="https://yumytest.oss-cn-chengdu.aliyuncs.com/img/image-20210316100705186.png" alt="image-20210316100705186" style="zoom:80%;" />

- Adagrad方法：

  - 更新原则：
    $$
    \boldsymbol{\theta}_{i}^{t+1} \leftarrow \boldsymbol{\theta}_{i}^{t}-\frac{\eta}{\sigma_{i}^{t}} \boldsymbol{g}_{i}^{t} \quad \sigma_{i}^{t}=\sqrt{\frac{1}{t+1} \sum_{i=0}^{t}\left(\boldsymbol{g}_{i}^{t}\right)^{2}}
    $$

  - 其中，g为梯度，$\eta$为学习率

  - 直观解释：缓梯度的时候，参数更小，学习率就更大

    <img src="https://yumytest.oss-cn-chengdu.aliyuncs.com/img/image-20210316101446560.png" alt="image-20210316101446560" style="zoom:50%;" />

  - 缺点：参数不随时间变化，不能动态调整

- RMSProp：

  - 思路：可以自己调整梯度的占比
    $$
    \begin{aligned}
    &\boldsymbol{\theta}_{i}^{1} \leftarrow \boldsymbol{\theta}_{i}^{0}-\frac{\eta}{\sigma_{i}^{0}} g_{i}^{0} \quad \sigma_{i}^{0}=\sqrt{\left(g_{i}^{0}\right)^{2}}\\
    &\boldsymbol{\theta}_{i}^{2} \leftarrow \boldsymbol{\theta}_{i}^{1}-\frac{\eta}{\sigma_{i}^{1}} g_{i}^{1} \quad \sigma_{i}^{1}=\sqrt{\alpha\left(\sigma_{i}^{0}\right)^{2}+(1-\alpha)\left(g_{i}^{1}\right)^{2}}\\
    &\boldsymbol{\theta}_{i}^{3} \leftarrow \boldsymbol{\theta}_{i}^{2}-\frac{\eta}{\sigma_{i}^{2}} g_{i}^{2} \quad \sigma_{i}^{2}=\sqrt{\alpha\left(\sigma_{i}^{1}\right)^{2}+(1-\alpha)\left(g_{i}^{2}\right)^{2}}\\
    &\boldsymbol{\theta}_{i}^{t+1} \leftarrow \boldsymbol{\theta}_{i}^{t}-\frac{\eta}{\sigma_{i}^{t}} \boldsymbol{g}_{i}^{t} \quad \sigma_{i}^{t}=\sqrt{\alpha\left(\sigma_{i}^{t-1}\right)^{2}+(1-\alpha)\left(\boldsymbol{g}_{\mathfrak{q}}^{t}\right)^{2}}
    \end{aligned}
    $$


- Adam：RMSProp+Momentum
- 学习率衰减decay
- warm up：学习率先增后减（resNet、Transformer）

### 优化总结

<img src="https://yumytest.oss-cn-chengdu.aliyuncs.com/img/image-20210316103252293.png" alt="image-20210316103252293" style="zoom: 50%;" />

## 分类（短版本）

- 用回归做：引入独热向量，每个类是一次回归

- 用分类区别：

  <img src="https://yumytest.oss-cn-chengdu.aliyuncs.com/img/image-20210316103847177.png" alt="image-20210316103847177" style="zoom:67%;" />

- Loss函数：基本都用Cross-entropy交叉熵，MSE也可以但是hui

- PS：pytorch里面，如果使用nn.CrossEntropyLoss() 则自动使用softmax而不需要添加softmax层



# CNN & Self-Attention

## CNN

- 背景：输入大小一样，输出为one-hot

- 已有的解决方法：将图片像素全部拉直成特征，喂到DNN中

- 观察1：通过找图中的patterns（我理解为：部分特征），然后进行提取

  <img src="https://yumytest.oss-cn-chengdu.aliyuncs.com/img/image-20210316145529979.png" alt="image-20210316145529979" style="zoom:67%;" />

  所以得到简化方法：分receptive field考虑，比如先考虑任意3x3x3

  引入基本概念：kernel size、channel、stride、padding、

- 观察2：同一个patterns可能出现在不同图片的不同地方

  简化方法：共享参数（filter相同）

- 卷积层：

  - 使用filter来抓取图像中的patterns

  - 图像通过filter得到的结果叫：feature map

  - 有多少filter，feature map就有多少channels

  - 多层卷积层的效果：如下图，假设上面矩阵（原图）用3x3卷积核，然后得到下面矩阵，如果再来一次卷积，则卷积的范围在原图中就更大一点（蓝色框）。也就是层越深，考虑的范围越大

    <img src="https://yumytest.oss-cn-chengdu.aliyuncs.com/img/image-20210316153948211.png" alt="image-20210316153948211" style="zoom:50%;" />

- 观察3：

  - 下采样subsampling：即缩小图片，比如可以间隔s个像素取出来生成新的图片

- 整个框架

  <img src="https://yumytest.oss-cn-chengdu.aliyuncs.com/img/image-20210316155023358.png" alt="image-20210316155023358" style="zoom:67%;" />

- 应用：下围棋

## self-attention

### intro

- 前面看到的输入都一样长，那么如果输入不一样长的序列会怎么办呢
  - 举例：输入序列this is a cat
    - 表示方法——独热向量：一个词占一个维度，但没有突出单词间的关系
    - word embedding：每个词一个向量（包含语义），同类词进行聚类【一句话就是长度不一的向量】
  - 举例：音频、图结构（社交网络）、分子结构
- 输出情况：
  - 一个向量对应一个label【sequence labeling】：如POS tagging（标词性）、声音识别（HW2）、社交网络图
  - 所有向量对应一个label：Sentiment analysis（情感分析）、语音辨认、判断分子是什么
  - 模型决定输出长度：【seq2seq】（HW5）

### 自注意力

https://www.youtube.com/watch?v=hYdO9CscNes

#### 先前做法

考虑输入输出一样的情况【sequence labeling】

- 先前做法：分别对每一个sequence进行FC，独立判断这个sequence的输出

- 但是sequence之间是有联系的，因此得考虑context

- 因此当前FC可以给当前和前后向量，如下图

  <img src="https://yumytest.oss-cn-chengdu.aliyuncs.com/img/image-20210326152046725.png" alt="image-20210326152046725" style="zoom:67%;" />

  问题：输入序列长度不一致，全面概括需要大量参数

#### 引入自注意力

<img src="https://yumytest.oss-cn-chengdu.aliyuncs.com/img/image-20210318151209129.png" alt="image-20210318151209129" style="zoom:67%;" />

- self-attention可交替/叠加使用（多次使用）

- 内部结构

  - a1~a4可能时输入层，也可能是隐藏层

  <img src="https://yumytest.oss-cn-chengdu.aliyuncs.com/img/image-20210318151415172.png" alt="image-20210318151415172" style="zoom:67%;" />

  - 那么怎么考虑b1与a1相关的向量之间得关联性呢?

- 评估相关程度$\alpha$的方法：

  - $\alpha$代表两个向量之间的关联程度

  - Dot-product（最常用）：向量乘W矩阵，得到qk点乘后得到α

  - Additive：

    <img src="https://yumytest.oss-cn-chengdu.aliyuncs.com/img/image-20210318151807291.png" alt="image-20210318151807291" style="zoom:67%;" />

- 具体做法：

  - 计算a1与a2~a4之间的关联性

    <img src="https://yumytest.oss-cn-chengdu.aliyuncs.com/img/image-20210318151949157.png" alt="image-20210318151949157" style="zoom:67%;" />

  - 一般自己跟自己也计算关联性（可实验）

  - 使用softmax进行normalization
  
    <img src="https://yumytest.oss-cn-chengdu.aliyuncs.com/img/image-20210318152052851.png" alt="image-20210318152052851" style="zoom:67%;" />
  
  - 基于attention分数抽取重要资讯：attention分数越大，在最终信息中占比就越大
  
    <img src="https://yumytest.oss-cn-chengdu.aliyuncs.com/img/image-20210318152339780.png" alt="image-20210318152339780" style="zoom:67%;" />
  
  ​	
  

# Theory of ML



















# Transformer









