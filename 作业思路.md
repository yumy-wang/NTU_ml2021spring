[toc]



# 第3次作业

作业地址：https://www.kaggle.com/c/ml2021spring-hw3/

作业介绍：https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/hw/HW03/HW03.pdf（不保证不会失效）

参考代码：https://colab.research.google.com/github/ga642381/ML2021-Spring/blob/main/HW03/HW03.ipynb

## 简要说明

使用CNN预测食物图片分类问题

- 目标：
  - 使用CNN
  - 数据增强
  - 处理无标签数据
- food-11数据集（11个类）
  - 训练集： 280 * 11 labeled images + 6786 unlabeled images
  - 验证集： 30 * 11 labeled images
  - 测试集：3347 images
- 要求：不能使用预训练模型（resnet等）
- baseline评判标准：准确度百分比

- 提示：

  - 半监督学习：为无标签数据生成伪标签（pseudo-labels）

    ![image-20210326154911569](https://yumytest.oss-cn-chengdu.aliyuncs.com/img/image-20210326154911569.png)




## 实作

### 改进点1——图像增强



```python
train_tfm = transforms.Compose([
    # Resize the image into a fixed shape (height = width = 128)
    transforms.Resize((128, 128)),
    # You may add some transforms here.
#     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
    transforms.RandomVerticalFlip(0.3),     # 以概率垂直翻转
    transforms.RandomHorizontalFlip(0.3),
    # ToTensor() should be the last one of the transforms.
    transforms.ToTensor(),
])
```

### 改进点2——模型结构

```python
class Classifier(nn.Module):
    def __init__(self):
        super(Classifier, self).__init__()
        # The arguments for commonly used modules:
        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)
        # torch.nn.MaxPool2d(kernel_size, stride, padding)

        # input image size: [3, 128, 128]
        self.cnn_layers = nn.Sequential(
            nn.Conv2d(3, 64, 3, 1, 1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2, 2, 0),

            nn.Conv2d(64, 128, 3, 1, 1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(2, 2, 0),

            nn.Conv2d(128, 256, 3, 1, 1),
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.MaxPool2d(4, 4, 0),
        )
        self.fc_layers = nn.Sequential(
            nn.Linear(256 * 8 * 8, 256),
            nn.ReLU(),
            nn.Linear(256, 64),
            nn.ReLU(),
            nn.Linear(64, 11)
        )
        
```

或者直接使用已有模型：

```python
model = torchvision.models.resnet18(pretrained=False).to(device)
```



### 改进点3——半监督学习

```python

```







## 问题

- [x] 为什么给无监督学习标记在训练模型之前

  

# 第5次作业

作业地址：校内competition

作业介绍：https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/hw/HW05/HW05.pdf（不保证不会失效）

参考代码：https://colab.research.google.com/github/ga642381/ML2021-Spring/blob/main/HW05/HW05_ZH.ipynb

参考视频：https://youtu.be/htG5WpZVQPU

## 简要说明

机器翻译任务：英转繁体中文

- 数据集

  1. TED2020：TED演讲的语言对（英-中）

  2. 只有中文的句子
- 要求/目标：
  - simple：使用sample即可通过
  - medium：换成transformer【变换结构和配置】
    - RNNEncoder -> TransformerEncoder
    - RNNDecoder -> TransformerDecoder
    - encoder_ffn_embed_dim -> 1024
    - encoder_layers/decoder_layers -> 4
  - hard：使用back-translation
    1. 训练zh-en的模型
    2. 使用transformer结构
    3. 将中文数据集输入得到英文数据，并获得合成数据集
    4. 使用合成数据集和原数据集进行训练（如果实现了，30个epochs即可实现）
- 评判标准：BLEU（基于准确率的句子相似度评估）

## tips

- Tokenize data with sub-word units：将sub-word作为最小单位（token），比如transportation可以切成trans+port+ation，这样做的好处可以减少总词汇数以及减少罕见词的影响
- Label smoothing regularization：让所有分类结果都得到重视，避免过拟合
- Learning rate scheduling：学习率先线性增加，后以平方根减小，让训练跟stable
- Back-translation（BT）：train一个中翻英的model，然后将只有中文的数据集放进去得到更多的数据用来训练模型，增加数据量
  - 数据集的背景要一致，是TED则都要演讲背景的数据



# 第7次作业

作业地址：https://www.kaggle.com/c/ml2021-spring-hw7【ddl：2021.05.21】

作业介绍：https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/hw/HW07/HW07.pdf（不保证不会失效）

参考代码：https://colab.research.google.com/github/ga642381/ML2021-Spring/blob/main/HW07/HW07.ipynb

参考视频：https://www.youtube.com/watch?v=DoZlp0vfDmI

## 简要说明

预训练bert模型，fine-tuning下游任务——基于提取的QA【给一段文字，给一个问题，答案从文字中找到】

- 数据集：中文阅读理解

  <img src="https://yumytest.oss-cn-chengdu.aliyuncs.com/img/image-20210507154757480.png" alt="image-20210507154757480" style="zoom: 67%;" />

  - 训练集：8k段话+27k问题
  - dev集：1k+3K5
  - 测试集：1k+3K5

- 要求/目标：

  - simple：sample
  - medium：学习率衰减+“doc_stride”值修改
  - strong：提高预处理+换模型
  - boss：

- 评判标准：精确匹配的准确率simple<medium<strong<boss

- 提交格式：{ID, Answer}

## tips

[toy example](./hw/hw7_tutorial.ipynb)

- bert最长输入512，因为时间复杂度为平方，而我们的数据集可能超过512怎么办？

  解决：设置**window**，（比如大小32）一段一段的来训练（可重叠）和测试（根据评分选择）

- 学习率衰减：

  - 手动调整：比如` optimizer.param_groups[0][“lr”] -= learning_rate / total training step`
  - 自动：[huggingface](https://huggingface.co/transformers/main_classes/optimizer_schedules.html#transformers.get_linear_schedule_with_warmup)，[pytorch](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)（https://www.jianshu.com/p/9643cba47655）

- Doc stride：两个连续window的开始位置之间的距离（默认为段落长）

- 预处理：答案不一定在window中间，怎么避免在边缘的情况（overlap）？

- 其他模型：只能在这里https://huggingface.co/models找。

- 后期处理：比如start>end

- Automatic mixed precision：加速训练【only work on some gpu (e.g. T4, V100)】

- **梯度累加(Gradient Accumulation)**：当我们做一些计算量需求大的任务(例如语义分割、GAN等)或者输入图片尺寸太大的时候，我们的batch size往往只能设置为2或者4，否则就会出现 “CUDA OUT OF MEMORY” 的不可抗力报错。https://cowarder.site/2019/10/29/Gradient-Accumulation/

  简言之，梯度累加就是每计算一个batch的梯度，不进行清零，而是做梯度的累加，当累加到一定的次数之后，再更新网络参数，然后将梯度清零。也就相当于手动做大的batch size

  参考code

```python
    for i, (inputs, labels) in enumerate(trainloader):
        outputs = net(inputs)                   # 正向传播
        loss = criterion(outputs, labels)       # 计算损失函数
        loss = loss / accumulation_steps        # 损失标准化
        loss.backward()                         # 反向传播，计算梯度
        if (i+1) % accumulation_steps == 0:
            optimizer.step()                    # 更新参数
            optimizer.zero_grad()               # 梯度清零
            if (i+1) % evaluation_steps == 0:
                evaluate_model()

```

## 改进部分

### 加速训练√

仅在特定gpu【colab pro的v100】上有效果，将fp16_training设置为True

```python
# Change "fp16_training" to True to support automatic mixed precision training (fp16)	
fp16_training = True

if fp16_training:
    !pip install accelerate==0.2.0
    from accelerate import Accelerator
    accelerator = Accelerator(fp16=True)
    device = accelerator.device

# Documentation for the toolkit:  https://huggingface.co/docs/accelerate/
```



### 改变doc_stride值 & Preprocessing√

Dataset and Dataloader部分：这个仅用于非train的时候

```python
##### TODO: Change value of doc_stride #####
        self.doc_stride = 32
```

这里预处理部分修改见注释，其中`ran = random.uniform(0.2, 0.8)`这里可以调整


```python
##### TODO: Preprocessing #####
        # Hint: How to prevent model from learning something it should not learn (i.e. answers are not always near the middle of window)
if self.split == "train":
            # Convert answer's start/end positions in paragraph_text to start/end positions in tokenized_paragraph  
            answer_start_token = tokenized_paragraph.char_to_token(question["answer_start"])
            answer_end_token = tokenized_paragraph.char_to_token(question["answer_end"])

            # A single window is obtained by slicing the portion of paragraph containing the answer
            # 原做法限定paragraph一定要包含answer，且尽量靠近中间，但问题是，这样做让模型更聚焦于answer处于中间的情况，而answer在test时不一定只在中间有
            # mid = (answer_start_token + answer_end_token) // 2
            # paragraph_start = max(0, min(mid - self.max_paragraph_len // 2, len(tokenized_paragraph) - self.max_paragraph_len))
            # paragraph_end = paragraph_start + self.max_paragraph_len
            
            # 新做法：
            ran = random.uniform(0.2, 0.8)
            mid = (int)(answer_start_token + ran * (answer_end_token - answer_start_token))
            paragraph_start = max(0, min(mid - self.max_paragraph_len // 2, len(tokenized_paragraph) - self.max_paragraph_len))
            paragraph_end = paragraph_start + self.max_paragraph_len
```



### Function for Evaluation部分改错和提升√

观察预测结果csv，有不认识的字符[unk]和空白，所以可以处理一下这两种情况

- 空白可以设置start>end时，翻转
- [unk]直接去掉吧

### 学习率衰减√

```python
learning_rate = 1e-4
optimizer = AdamW(model.parameters(), lr=learning_rate)
scheduler = get_linear_schedule_with_warmup(optimizer, 0, len(train_loader))

...
for epoch in range(num_epoch):
    step = 1
	...
    for data in tqdm(train_loader):
        ##### TODO: Apply linear learning rate decay #####
        optimizer.step()
        scheduler.step()

```

打印效果：

```shell
lr: 9.412114014251782e-05
lr: 8.818289786223278e-05
lr: 8.224465558194774e-05
lr: 7.630641330166272e-05
lr: 7.036817102137767e-05
lr: 6.442992874109263e-05
lr: 5.84916864608076e-05
lr: 5.255344418052257e-05
lr: 4.661520190023753e-05
lr: 4.06769596199525e-05
lr: 3.473871733966746e-05
lr: 2.8800475059382425e-05
lr: 2.2862232779097388e-05
lr: 1.6923990498812355e-05
lr: 1.0985748218527318e-05
lr: 5.047505938242281e-06
```



### 改变模型√

这里我没有细纠，直接使用BertForQuestionAnswering和BertTokenizerFast修改后面的模型名字做的，他给的模型网站是用的Autoxxx来做，会报错`TypeError: forward() got an unexpected keyword argument`

```python
model_name = 'xxxxxx'
model = BertForQuestionAnswering.from_pretrained(model_name).to(device)
tokenizer = BertTokenizerFast.from_pretrained(model_name)
```

### 超参数

- 模型选择model_name
- 窗口大小self.doc_stride
- 范围限定ran = random.uniform(0, 1)
- train_batch_size
- learning_rate

### 总结

已经尽力了还是没过strong baseline，用了macbert模型，调整doc_stride=8，epoch=2了，还是无法提升55555~

## leaderboard

见[here](./README.md)

## 其他

一些好的教程：

- https://www.cnblogs.com/cxq1126/p/13517394.html



# 第8次作业

作业地址：https://www.kaggle.com/c/ml2021spring-hw8【ddl：2021.05.21】

作业介绍：https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/hw/HW08/HW08.pdf（不保证不会失效）

参考代码：https://colab.research.google.com/github/ga642381/ML2021-Spring/blob/main/HW08/HW08.ipynb

参考视频：https://youtu.be/xkpXP4byXqk

## 简要说明

使用autoencoder判断一个机器学习模型是否能检测测试图片与训练图片是同一个类别/分布

- 数据集：

  - 训练集：140k人脸图片（64x64x3）
  - 测试集：10k张人脸图片（标记为0），10k张另外分布的人脸（标记为1）

- 要求/目标：

  - simple：FCN autoencoder
  - medium：CNN autoencoder + 更小的模型、层数 + 更小的batch size
  - strong：Add BatchNorm + train 更久
  - boss：加额外的分类器（discriminator） + 给异常图片添加随机噪声 +  OCGAN

-  评判标准：需要设置门槛（分数）来判断是否异常，但这样就需要手动去找这个值，我们想的是从模型本身出发，找到一个敏感的探测器，将正常和异常能明显分开

  ROC_AUC分数：含义是sensor下半部分的面积，越高越好，会经过正则化，因此最大为1。计算方法（ppt里面p15~p17）：先由分数排序，计算TP和FP，然后正则化，最后计算面积

- 提交格式：id，score



## 改进部分

### 模型部分

给了四种模型：FCN、CNN、VAE、resnet，可以自行调整其参数

#### FCN

没啥需要改的

#### CNN

- 减少层数
- 调整channel
- 加bn层

```python
class conv_autoencoder(nn.Module):
    def __init__(self):
        super(conv_autoencoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 12, 4, stride=2, padding=1), 
            nn.BatchNorm2d(12),        
            nn.ReLU(),
            nn.Conv2d(12, 24, 4, stride=2, padding=1),  
            nn.BatchNorm2d(24),       
            nn.ReLU(),
			nn.Conv2d(24, 48, 4, stride=2, padding=1),         
            nn.BatchNorm2d(48), 
            nn.ReLU(),
            # nn.Conv2d(48, 96, 4, stride=2, padding=1),   # medium: remove this layer  4x4x96
            # nn.ReLU(),
        )
        self.decoder = nn.Sequential(
            # nn.ConvTranspose2d(96, 48, 4, stride=2, padding=1), # medium: remove this layer
            # nn.ReLU(),
			nn.ConvTranspose2d(48, 24, 4, stride=2, padding=1), 
            nn.BatchNorm2d(24), 
            nn.ReLU(),
			nn.ConvTranspose2d(24, 12, 4, stride=2, padding=1), 
            nn.BatchNorm2d(12), 
            nn.ReLU(),
            nn.ConvTranspose2d(12, 3, 4, stride=2, padding=1),
            nn.BatchNorm2d(3), 
            nn.Tanh(),
        )

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x

```



#### VAE



#### resnet



### 学习率衰减√

```python
optimizer = torch.optim.Adam(
    model.parameters(), lr=learning_rate)
# 学习率衰减
scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.9)

for epoch in qqdm_train:
    for data in train_dataloader:
		...
        optimizer.step()
    scheduler.step()
    print(optimizer.param_groups[0]['lr'] )
```



















